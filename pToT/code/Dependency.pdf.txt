7
1
0
2

 
r
a

 

M
0
1

 
 
]
L
C
.
s
c
[
 
 

3
v
4
3
7
1
0

.

1
1
6
1
:
v
i
X
r
a

Published as a conference paper at ICLR 2017

DEEP BIAFFINE ATTENTION FOR NEURAL
DEPENDENCY PARSING

Timothy Dozat
Stanford University
tdozat@stanford.edu

Christopher D. Manning
Stanford University
manning@stanford.edu

ABSTRACT

This paper builds off recent work from Kiperwasser & Goldberg (2016) using neu-
ral attention in a simple graph-based dependency parser. We use a larger but more
thoroughly regularized parser than other recent BiLSTM-based approaches, with
biafﬁne classiﬁers to predict arcs and labels. Our parser gets state of the art or
near state of the art performance on standard treebanks for six different languages,
achieving 95.7% UAS and 94.1% LAS on the most popular English PTB dataset.
This makes it the highest-performing graph-based parser on this benchmark—
outperforming Kiperwasser & Goldberg (2016) by 1.8% and 2.2%—and com-
parable to the highest performing transition-based parser (Kuncoro et al., 2016),
which achieves 95.8% UAS and 94.6% LAS. We also show which hyperparameter
choices had a signiﬁcant effect on parsing accuracy, allowing us to achieve large
gains over other graph-based approaches.

1

INTRODUCTION

Dependency parsers—which annotate sentences in a way designed to be easy for humans and com-
puters alike to understand—have been found to be extremely useful for a sizable number of NLP
tasks, especially those involving natural language understanding in some way (Bowman et al., 2016;
Angeli et al., 2015; Levy & Goldberg, 2014; Toutanova et al., 2016; Parikh et al., 2015). However,
frequent incorrect parses can severely inhibit ﬁnal performance, so improving the quality of depen-
dency parsers is needed for the improvement and success of these downstream tasks.

The current state-of-the-art transition-based neural dependency parser (Kuncoro et al., 2016) sub-
stantially outperforms many much simpler neural graph-based parsers. We modify the neural graph-
based approach ﬁrst proposed by Kiperwasser & Goldberg (2016) in a few ways to achieve com-
petitive performance: we build a network that’s larger but uses more regularization; we replace the
traditional MLP-based attention mechanism and afﬁne label classiﬁer with biafﬁne ones; and rather
than using the top recurrent states of the LSTM in the biafﬁne transformations, we ﬁrst put them
through MLP operations that reduce their dimensionality. Furthermore, we compare models trained
with different architectures and hyperparameters to motivate our approach empirically. The result-
ing parser maintains most of the simplicity of neural graph-based approaches while approaching the
performance of the SOTA transition-based one.

2 BACKGROUND AND RELATED WORK

Transition-based parsers—such as shift-reduce parsers—parse sentences from left to right, main-
taining a “buffer” of words that have not yet been parsed and a “stack” of words whose head has not
been seen or whose dependents have not all been fully parsed. At each step, transition-based parsers
can access and manipulate the stack and buffer and assign arcs from one word to another. One can
then train any multi-class machine learning classiﬁer on features extracted from the stack, buffer,
and previous arc actions in order to predict the next action.

Chen & Manning (2014) make the ﬁrst successful attempt at incorporating deep learning into a
transition-based dependency parser. At each step, the (feedforward) network assigns a probability
to each action the parser can take based on word, tag, and label embeddings from certain words

1

Published as a conference paper at ICLR 2017

root

nsubj

dobj

root/ROOT Casey/NNP hugged/VBD Kim/NNP

Figure 1: A dependency tree parse for Casey hugged Kim, including part-of-speech tags and a special
root token. Directed edges (or arcs) with labels (or relations) connect the verb to the root and the
arguments to the verb head.

on the stack and buffer. A number of other researchers have attempted to address some limita-
tions of Chen & Manning’s Chen & Manning parser by augmenting it with additional complexity:
Weiss et al. (2015) and Andor et al. (2016) augment it with a beam search and a conditional random
ﬁeld loss objective to allow the parser to “undo” previous actions once it ﬁnds evidence that they may
have been incorrect; and Dyer et al. (2015) and (Kuncoro et al., 2016) instead use LSTMs to rep-
resent the stack and buffer, getting state-of-the-art performance by building in a way of composing
parsed phrases together.

Transition-based parsing processes a sentence sequentially to build up a parse tree one arc at a
time. Consequently, these parsers don’t use machine learning for directly predicting edges; they
use it for predicting the operations of the transition algorithm. Graph-based parsers, by contrast,
use machine learning to assign a weight or probability to each possible edge and then construct a
maximum spaning tree (MST) from these weighted edges. Kiperwasser & Goldberg (2016) present
a neural graph-based parser (in addition to a transition-based one) that uses the same kind of attention
mechanism as Bahdanau et al. (2014) for machine translation. In Kiperwasser & Goldberg’s 2016
model, the (bidirectional) LSTM’s recurrent output vector for each word is concatenated with each
possible head’s recurrent vector, and the result is used as input to an MLP that scores each resulting
arc. The predicted tree structure at training time is the one where each word depends on its highest-
scoring head. Labels are generated analogously, with each word’s recurrent output vector and its
gold or predicted head word’s recurrent vector being used in a multi-class MLP.

Similarly, Hashimoto et al. (2016) include a graph-based dependency parser in their multi-task neu-
ral model. In addition to training the model with multiple distinct objectives, they replace the tra-
ditional MLP-based attention mechanism that Kiperwasser & Goldberg (2016) use with a bilinear
one (but still using an MLP label classiﬁer). This makes it analogous to Luong et al.’s 2015 pro-
posed attention mechanism for neural machine translation. Cheng et al. (2016) likewise propose a
graph-based neural dependency parser, but in a way that attempts to circumvent the limitation of
other neural graph-based parsers being unable to condition the scores of each possible arc on pre-
vious parsing decisions. In addition to having one bidirectional recurrent network that computes a
recurrent hidden vector for each word, they have additional, unidirectional recurrent networks (left-
to-right and right-to-left) that keep track of the probabilities of each previous arc, and use these
together to predict the scores for the next arc.

3 PROPOSED DEPENDENCY PARSER

3.1 DEEP BIAFFINE ATTENTION

We make a few modiﬁcations to the graph-based architectures of Kiperwasser & Goldberg (2016),
Hashimoto et al. (2016), and Cheng et al. (2016), shown in Figure 2: we use biafﬁne attention in-
stead of bilinear or traditional MLP-based attention; we use a biafﬁne dependency label classiﬁer;
and we apply dimension-reducing MLPs to each recurrent output vector ri before applying the bi-
afﬁne transformation.1 The choice of biafﬁne rather than bilinear or MLP mechanisms makes the
classiﬁers in our model analogous to traditional afﬁne classiﬁers, which use an afﬁne transformation
over a single LSTM output state ri (or other vector input) to predict the vector of scores si for all
classes (1). We can think of the proposed biafﬁne attention mechanism as being a traditional afﬁne

1In this paper we follow the convention of using lowercase italic letters for scalars and indices, lowercase
bold letters for vectors, uppercase italic letters for matrices, uppercase bold letters for higher order tensors. We
also maintain this notation when indexing; so row i of matrix R would be represented as ri.

2

Published as a conference paper at ICLR 2017

H (arc-dep) ⊕ 1

U (arc)

H (arc-head)

S(arc)

1

1

1

1

·

·

⊤

=

MLP: h(arc-dep)

i

, h(arc-head)

i

BiLSTM: ri

Embeddings: xi

. . .

root ROOT

Kim NNP

Figure 2: BiLSTM with deep biafﬁne attention to score each possible head for each dependent,
applied to the sentence “Casey hugged Kim”. We reverse the order of the biafﬁne transformation
here for clarity.

classiﬁer, but using a (d × d) linear transformation of the stacked LSTM output RU (1) in place of
the weight matrix W and a (d × 1) transformation Ru(2) for the bias term b (2).

si = W ri + b

s(arc)
i

= (cid:16)RU (1)(cid:17) ri + (cid:16)Ru(2)(cid:17)

Fixed-class afﬁne classiﬁer

Variable-class biafﬁne classiﬁer

(1)

(2)

In addition to being arguably simpler than the MLP-based approach (involving one bilinear layer
rather than two linear layers and a nonlinearity), this has the conceptual advantage of directly mod-
u(2) and the
eling both the prior probability of a word j receiving any dependents in the term r⊤
j
j U (1)ri. Analogously, we also use a
likelihood of j receiving a speciﬁc dependent i in the term r⊤
biafﬁne classiﬁer to predict dependency labels given the gold or predicted head yi (3).

s(label)
i

= r⊤
yi

U(1)ri + (ryi ⊕ ri)⊤U (2) + b

Fixed-class biafﬁne classiﬁer

(3)

This likewise directly models each of the prior probability of each class, the likelihood of a class
given just word i (how probable a word is to take a particular label), the likelihood of a class given
just the head word yi (how probable a word is to take dependents with a particular label), and the
likelihood of a class given both word i and its head (how probable a word is to take a particular label
given that word’s head).

Applying smaller MLPs to the recurrent output states before the biafﬁne classiﬁer has the advantage
of stripping away information not relevant to the current decision. That is, every top recurrent state
ri will need to carry enough information to identify word i’s head, ﬁnd all its dependents, exclude all
its non-dependents, assign itself the correct label, and assign all its dependents their correct labels, as
well as transfer any relevant information to the recurrent states of words before and after it. Thus ri
necessarily contains signiﬁcantly more information than is needed to compute any individual score,
and training on this superﬂuous information needlessly reduces parsing speed and increases the risk
of overﬁtting. Reducing dimensionality and applying a nonlinearity (4 - 6) addresses both of these
problems. We call this a deep bilinear attention mechanism, as opposed to shallow bilinear attention,
which uses the recurrent states directly.
h(arc-dep)

(4)

(5)

(6)

i

h(arc-head)

j

s(arc)
i

= MLP(arc-dep)(ri)
= MLP(arc-head)(rj)
= H (arc-head)U (1)h(arc-dep)

i
+ H (arc-head)u(2)

We apply MLPs to the recurrent states before using them in the label classiﬁer as well. As with other
graph-based models, the predicted tree at training time is the one where each word is a dependent of
its highest scoring head (although at test time we ensure that the parse is a well-formed tree via the
MST algorithm).

3

Published as a conference paper at ICLR 2017

3.2 HYPERPARAMETER CONFIGURATION

Param
Embedding size
LSTM size
Arc MLP size
Label MLP size
LSTM depth
α

Annealing

Value
100
400
500
100
3
2e−3
.75

t

5000

Param
Embedding dropout
LSTM dropout
Arc MLP dropout
Label MLP dropout
MLP depth
β1,β2
tmax

Value
33%
33%
33%
33%
1
.9
50,000

Table 1: Model hyperparameters

Aside from architectural differences between ours and the other graph-based parsers, we make a
number of hyperparameter choices that allow us to outperform theirs, laid out in Table 1. We use
100-dimensional uncased word vectors2 and POS tag vectors; three BiLSTM layers (400 dimensions
in each direction); and 500- and 100-dimensional ReLU MLP layers. We also apply dropout at every
stage of the model: we drop words and tags (independently); we drop nodes in the LSTM layers
(input and recurrent connections), applying the same dropout mask at every recurrent timestep (cf.
the Bayesian dropout of Gal & Ghahramani (2015)); and we drop nodes in the MLP layers and
classiﬁers, likewise applying the same dropout mask at every timestep. We optimize the network
with annealed Adam (Kingma & Ba, 2014) for about 50,000 steps, rounded up to the nearest epoch.

4 EXPERIMENTS & RESULTS

4.1 DATASETS

We show test results for the proposed model on the English Penn Treebank, converted into Stanford
Dependencies using both version 3.3.0 and version 3.5.0 of the Stanford Dependency converter
(PTB-SD 3.3.0 and PTB-SD 3.5.0); the Chinese Penn Treebank; and the CoNLL 09 shared task
dataset,3 following standard practices for each dataset. We omit punctuation from evaluation only
for the PTB-SD and CTB. For the English PTB-SD datasets, we use POS tags generated from the
Stanford POS tagger (Toutanova et al., 2003); for the Chinese PTB dataset we use gold tags; and for
the CoNLL 09 dataset we use the provided predicted tags. Our hyperparameter search was done with
the PTB-SD 3.5.0 validation dataset in order to minimize overﬁtting to the more popular PTB-SD
3.3.0 benchmark, and in our hyperparameter analysis in the following section we report performance
on the PTB-SD 3.5.0 test set, shown in Tables 2 and 3.

4.2 HYPERPARAMETER CHOICES

4.2.1 ATTENTION MECHANISM

We examined the effect of different classiﬁer architectures on accuracy and performance. What we
see is that the deep bilinear model outperforms the others with respect to both speed and accuracy.
The model with shallow bilinear arc and label classiﬁers gets the same unlabeled performance as the
deep model with the same settings, but because the label classiﬁer is much larger ((801 × c × 801) as
opposed to (101 × c × 101)), it runs much slower and overﬁts. One way to decrease this overﬁtting
is by increasing the MLP dropout, but that of course doesn’t change parsing speed; another way is
to decrease the recurrent size to 300, but this hinders unlabeled accuracy without increasing parsing
speed up to the same levels as our deeper model. We also implemented the MLP-based approach
to attention and classiﬁcation used in Kiperwasser & Goldberg (2016).4 We found this version to

2We compute a “trained” embedding matrix composed of words that occur at least twice in the training
dataset and add these embeddings to their corresponding pretrained embeddings. Any words that don’t occur
in either embedding matrix are replaced with a separate OOV token.

3We exclude the Japanese dataset from our evaluation because we do not have access to it.
4In the version of TensorFlow we used, the model’s memory requirements during training exceeded the
available memory on a single GPU when default settings were used, so we reduced the MLP hidden size to 200

4

Published as a conference paper at ICLR 2017

Model
Deep
Shallow
Shallow, 50% drop
Shallow, 300d
MLP

Classiﬁer
UAS
95.75
95.74
95.73
95.63*
95.53*

LAS
94.22
94.00*
94.05*
93.86*
93.91*

Size

Sents/sec
410.91
298.99
300.04
373.24
367.44

Model
3 layers, 400d
3 layers, 300d
3 layers, 200d
2 layers, 400d
4 layers, 400d

UAS
95.75
95.82
95.55*
95.62*
95.83

LAS
94.22
94.24
93.89*
93.98*
94.22

Sents/sec
410.91
460.01
469.45
497.99
362.09

Model
LSTM
GRU
Cif-LSTM

Recurrent Cell

UAS
95.75
93.18*
95.67

LAS
94.22
91.08*
94.06*

Sents/sec
410.91
435.32
463.25

Table 2: Test accuracy and speed on PTB-SD 3.5.0. Statistically signiﬁcant differences are marked
with an asterisk.

Input Dropout

Model
Default
No word dropout
No tag dropout
No tags

UAS
95.75
95.74
95.28*
95.77

LAS
94.22
94.08*
93.60*
93.91*

Model
β2 = .9
β2 = .999

Adam
UAS
95.75
95.53*

LAS
94.22
93.91*

Table 3: Test Accuracy on PTB-SD 3.5.0. Statistically signiﬁcant differences are marked with an
asterisk.

likewise be somewhat slower and signiﬁcantly underperform the deep biafﬁne approach in both
labeled and unlabeled accuracy.

4.2.2 NETWORK SIZE

We also examine more closely how network size inﬂuences speed and accuracy.
In
Kiperwasser & Goldberg’s 2016 model, the network uses 2 layers of 125-dimensional bidirectional
LSTMs; in Hashimoto et al.’s 2016 model, it has one layer of 100-dimensional bidirectional LSTMs
dedicated to parsing (two lower layers are also trained on other objectives); and Cheng et al.’s 2016
model has one layer of 368-dimensional GRU cells. We ﬁnd that using three or four layers gets
signiﬁcantly better performance than two layers, and increasing the LSTM sizes from 200 to 300 or
400 dimensions likewise signﬁcantly improves performance.5

4.2.3 RECURRENT CELL

GRU cells have been promoted as a faster and simpler alternative to LSTM cells, and are used in
the approach of Cheng et al. (2016); however, in our model they drastically underperformed LSTM
cells. We also implemented the coupled input-forget gate LSTM cells (Cif-LSTM) suggested by
Greff et al. (2015),6 ﬁnding that while the resulting model still slightly underperforms the more
popular LSTM cells, the difference between the two is much smaller. Additionally, because the
gate and candidate cell activations can be computed simultaneously with one matrix multiplication,
the Cif-LSTM model is faster than the GRU version even though they have the same number of
parameters. We hypothesize that the output gate in the Cif-LSTM model allows it to maintain a
sparse recurrent output state, which helps it adapt to the high levels of dropout needed to prevent
overﬁtting in a way that GRU cells are unable to do.

5The model with 400-dimensional recurrent states signiﬁcantly outperforms the 300-dimensional one on

the validation set, but not on the test set

6In addition to using a coupled input-forget gate, we remove the ﬁrst tanh nonlinearity, which is no longer

needed when using a coupled gate

5

Published as a conference paper at ICLR 2017

Type

Model

Transition

Ballesteros et al. (2016)
Andor et al. (2016)
Kuncoro et al. (2016)

Graph

Kiperwasser & Goldberg (2016)
Cheng et al. (2016)
Hashimoto et al. (2016)
Deep Biafﬁne

English PTB-SD 3.3.0 Chinese PTB 5.1
UAS

LAS

UAS

LAS

93.56
94.61
95.8

93.9
94.10
94.67
95.74

91.42
92.79
94.6

91.9
91.49
92.90
94.08

87.65
–
–

87.6
88.1
–
89.30

86.21
–
–

86.1
85.7
–
88.23

Table 4: Results on the English PTB and Chinese PTB parsing datasets

Model

UAS

LAS

UAS

LAS

UAS

LAS

Catalan

Chinese

Czech

Andor et al.
Deep Biafﬁne

92.67
94.69

89.83
92.02

84.72
88.90

80.85
85.38

88.94
92.08

84.56
87.38

Model

UAS

LAS

UAS

LAS

UAS

LAS

English

German

Spanish

Andor et al.
Deep Biafﬁne

93.22
95.21

91.23
93.20

90.91
93.46

89.15
91.44

92.62
94.34

89.95
91.65

Table 5: Results on the CoNLL ’09 shared task datasets

4.2.4 EMBEDDING DROPOUT

Because we increase the parser’s power, we also have to increase its regularization. In addition to
using relatively extreme dropout in the recurrent and MLP layers mentioned in Table 1, we also
regularize the input layer. We drop 33% of words and 33% of tags during training: when one is
dropped the other is scaled by a factor of two to compensate, and when both are dropped together,
the model simply gets an input of zeros. Models trained with only word or tag dropout but not
both wind up signﬁcantly overﬁtting, hindering label accuracy and—in the latter case—attachment
accuracy. Interestingly, not using any tags at all actually results in better performance than using
tags without dropout.

4.2.5 OPTIMIZER

We choose to optimize with Adam (Kingma & Ba, 2014), which (among other things) keeps a mov-
ing average of the L2 norm of the gradient for each parameter throughout training and divides the
gradient for each parameter by this moving average, ensuring that the magnitude of the gradients will
on average be close to one. However, we ﬁnd that the value for β2 recommended by Kingma & Ba—
which controls the decay rate for this moving average—is too high for this task (and we suspect more
generally). When this value is very large, the magnitude of the current update is heavily inﬂuenced
by the larger magnitude of gradients very far in the past, with the effect that the optimizer can’t adapt
quickly to recent changes in the model. Thus we ﬁnd that setting β2 to .9 instead of .999 makes a
large positive impact on ﬁnal performance.

4.3 RESULTS

Our model gets nearly the same UAS performance on PTB-SD 3.3.0 as the current SOTA model
from Kuncoro et al. (2016) in spite of its substantially simpler architecture, and gets SOTA UAS
performance on CTB 5.17 as well as SOTA performance on all CoNLL 09 languages. It is worth
noting that the CoNLL 09 datasets contain many non-projective dependencies, which are difﬁcult
or impossible for transition-based—but not graph-based—parsers to predict. This may account for
some of the large, consistent difference between our model and Andor et al.’s 2016 transition-based
model applied to these datasets.

7We’d like to thank Zhiyang Teng for ﬁnding a bug in the original code that affected the CTB 5.1 dataset

6

Published as a conference paper at ICLR 2017

Where our model appears to lag behind the SOTA model is in LAS, indicating one of a few possibil-
ities. Firstly, it may be the result of inefﬁciencies or errors in the GloVe embeddings or POS tagger,
in which case using alternative pretrained embeddings or a more accurate tagger might improve
label classiﬁcation. Secondly, the SOTA model is speciﬁcally designed to capture phrasal composi-
tionality; so another possibility is that ours doesn’t capture this compositionality as effectively, and
that this results in a worse label score. Similarly, it may be the result of a more general limitation of
graph-based parsers, which have access to less explicit syntactic information than transition-based
parsers when making decisions. Addressing these latter two limitations would require a more inno-
vative architecture than the relatively simple one used in current neural graph-based parsers.

5 CONCLUSION

In this paper we proposed using a modiﬁed version of bilinear attention in a neural dependency
parser that increases parsing speed without hurting performance. We showed that our larger but more
regularized network outperforms other neural graph-based parsers and gets comparable performance
to the current SOTA transition-based parser. We also provided empirical motivation for the proposed
architecture and conﬁguration over similar ones in the existing literature. Future work will involve
exploring ways of bridging the gap between labeled and unlabeled accuracy and augment the parser
with a smarter way of handling out-of-vocabulary tokens for morphologically richer languages.

REFERENCES

Daniel Andor, Chris Alberti, David Weiss, Aliaksei Severyn, Alessandro Presta, Kuz-
Globally normalized transition-
URL

In Association for Computational Linguistics, 2016.

man Ganchev, Slav Petrov,
based neural networks.
https://arxiv.org/abs/1603.06042.

and Michael Collins.

Gabor Angeli, Melvin Johnson Premkumar, and Christopher D Manning. Leveraging linguistic
structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of
the Association for Computational Linguistics (ACL 2015), 2015.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly

learning to align and translate. International Conference on Learning Representations, 2014.

Miguel Ballesteros, Yoav Goldberg, Chris Dyer, and Noah A Smith. Training with exploration
improves a greedy stack-LSTM parser. Proceedings of the conference on empirical methods in
natural language processing, 2016.

Samuel R Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D Manning, and
Christopher Potts. A fast uniﬁed model for parsing and sentence understanding. ACL 2016, 2016.

Danqi Chen and Christopher D Manning. A fast and accurate dependency parser using neural
networks. In Proceedings of the conference on empirical methods in natural language processing,
pp. 740–750, 2014.

Hao Cheng, Hao Fang, Xiaodong He, Jianfeng Gao, and Li Deng. Bi-directional attention with

agreement for dependency parsing. arXiv preprint arXiv:1608.02076, 2016.

Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A Smith. Transition-
based dependency parsing with stack long short-term memory. Proceedings of the conference on
empirical methods in natural language processing, 2015.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model

uncertainty in deep learning. International Conference on Machine Learning, 2015.

Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn´ık, Bas R Steunebrink, and J¨urgen Schmidhuber.
LSTM: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems,
2015.

7

Published as a conference paper at ICLR 2017

Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task
model: Growing a neural network for multiple nlp tasks. arXiv preprint arXiv:1611.01587, 2016.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

International

Conference on Learning Representations, 2014.

Eliyahu Kiperwasser and Yoav Goldberg. Simple and accurate dependency parsing using bidirec-
tional LSTM feature representations. Transactions of the Association for Computational Linguis-
tics, 4:313–327, 2016.

Adhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham Neubig, and Noah A.
Smith. What do recurrent neural network grammars learn about syntax? CoRR, abs/1611.05774,
2016. URL http://arxiv.org/abs/1611.05774.

Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In ACL 2014, pp. 302–308,

2014.

Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-

based neural machine translation. Empirical Methods in Natural Language Processing, 2015.

Ankur P Parikh, Hoifung Poon, and Kristina Toutanova. Grounded semantic parsing for complex
knowledge extraction. In Proceedings of North American Chapter of the Association for Compu-
tational Linguistics, pp. 756–766, 2015.

Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. Feature-rich part-of-
speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Computational Linguistics on Human Language
Technology-Volume 1, pp. 173–180. Association for Computational Linguistics, 2003.

Kristina Toutanova, Xi Victoria Lin, and Wen-tau Yih. Compositional learning of embeddings for

relation paths in knowledge bases and text. In ACL, 2016.

David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. Structured training for neural network
transition-based parsing. Annual Meeting of the Association for Computational Linguistics, 2015.

8

